{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fdLY2IbJu5_"
   },
   "source": [
    "<h1><center>Big Data Algorithms Techniques & Platforms</center></h1>\n",
    "\n",
    "<h2>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<center>Assignment 2: Introduction to Spark</center>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GD3rT2CvJu6A"
   },
   "source": [
    "# 1. Introduction\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In this set of exercises you'll learn basic Spark programming skills that are necessary to develop simple, yet powerful, applications to be executed in a distributed environment.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The assignment is presented in this __Jupyter Notebook__, an interface that offers support for text, code, images and other media. Essentially, a Jupyter Notebook consists of multiple _cells_, either containing some text, like the one that you are reading, or code that you can execute. \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization successful!\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local\")\n",
    "sc = SparkContext(conf = conf)\n",
    "print(\"initialization successful!\")\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import os\n",
    "seed_value = 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BB7DUrvD5kCR"
   },
   "source": [
    "# B. Data import\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Upload the folder data.zip inside the colab data folder and then execute the following code.\n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbHqimGOG8mD"
   },
   "source": [
    "# C. Support functions \n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Some support functions are provided. Read carefully the signatures of the fuctions.\n",
    "<ul>\n",
    "<li> $remove\\_non\\_letters(word)$\n",
    "<li> $load\\_stopwords(stopwords\\_file)$\n",
    "<li> $preprocess(text, stopwords)$\n",
    "<li> $word\\_count(words)$\n",
    "</ul>\n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "08jVdk-T7JeF"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# Regular expression for removing all non-letter characters in the file.\n",
    "regex = re.compile('[^a-zA-Z ]')\n",
    "\n",
    "\n",
    "'''\n",
    "Removes any non-letter character from the given word.\n",
    "\n",
    "INPUT:\n",
    "        word: A word\n",
    "\n",
    "OUTPUT:\n",
    "        the input word without the non-letter characters.\n",
    "\n",
    "'''\n",
    "def remove_non_letters(word):\n",
    "    return regex.sub('', word)\n",
    "\n",
    "\n",
    "'''\n",
    "INPUT: \n",
    "        stopwords_file: name of the file containing the stopwords.\n",
    "OUTPUT:\n",
    "        a Python list with the stopwords read from the file.\n",
    "'''\n",
    "def load_stopwords(stopwords_file):\n",
    "    stopwords = []\n",
    "    with open(stopwords_file) as file:\n",
    "        for sw in file:\n",
    "            stopwords.append(sw.strip())\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "'''\n",
    "INPUT: \n",
    "        text: RDD where each element is a line of the input text file.\n",
    "        stopwords: Python list containing the stopwords.\n",
    "OUTPUT: \n",
    "        RDD where each element is a word from the input text file.\n",
    "'''\n",
    "def preprocess(text, stopwords) :\n",
    "  words = text.flatMap(lambda line: line.split(\" \")).map(lambda word: remove_non_letters(word)).filter(lambda word: len(word) > 0).map(lambda word: word.lower()).filter(lambda word: word not in stopwords)\n",
    "  return words\n",
    "\n",
    "'''\n",
    "Returns how many times a word appears in a RDD \n",
    "INPUT:\n",
    "        words: RDD, where each element is word from the input text file (preprocessing already done!).\n",
    "OUTPUT:\n",
    "        RDD, where each element is (w, occ), w is a word and occ the number of occurrences of w.\n",
    "        The RDD is sorted by value in decreasing order.\n",
    "'''\n",
    "\n",
    "def word_count(words):    \n",
    "    occs = words.map(lambda word: (word, 1))\\\n",
    "                .reduceByKey(lambda x, y: x+y)\\\n",
    "                .sortBy(lambda f: f[1], ascending=False)\n",
    "    return occs\n",
    "\n",
    "# Storing in stopwords the list of the stopwords that is provided\n",
    "stopwords = load_stopwords(\"./data/stopwords.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdXYc3aMJu6x"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<hr style=\"border:solid 2px;\">\n",
    "\n",
    "##  Exercise 1\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In the folder _./data/bbc_ you'll find a collection of 50 documents from the BBC news website corresponding to stories in five topics. The five topics are:\n",
    "<ul>\n",
    "<li> _business_ \n",
    "<li>_entertainment_\n",
    "<li> _politics_\n",
    "<li> _sport_ \n",
    "<li> _tech_\n",
    "</ul>\n",
    "\n",
    "In the directory, the stories are text files (named: $\\_001.txt\\_$, $\\_002.txt\\_$, ...) organized into five directories, one for topic.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In this exercise, we want to create an **inverted index**. An inverted index is an essential component of a search engine. In fact, given any word, the inverted index allows the search engine to quickly retrieve all documents containing that word.\n",
    "\n",
    "An inverted index associates each word (you can find in the files) to the list of the names files the word occurs in.\n",
    "\n",
    "More precisely, for each word, the inverted index will have a list of the names  of the files (path relative to the folder _./data_) that contain the word. \n",
    "\n",
    "\n",
    "(family, \\[./data/bbc/tech/006.txt, ./data/bbc/entertainment/003.txt, ./data/bbc/entertainment/005.txt, ...\\]\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The function $inverted\\_index$ has the following input and output:\n",
    "<ul>\n",
    "    <li> **Input.** A RDD $files$, where each element is $(f, content)$, $f$ being the name of a text file in the collection and $content$ being the content of that file; \n",
    "a Python list $stopwords$, containing the most common English stopwords.\n",
    "    <li> **Output.** A RDD, where each element is $(w, L)$, $w$ is a word and $L$ is the list of the names of the files containing $w$. The list must not contain duplicate file names.\n",
    "</ul>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Write the code of the function $inverted\\_index()$. The function must apply a sequence of RDD transformations to:**\n",
    "<ol>\n",
    "  <li> split the content of each file into its constituent words.\n",
    "  <li> lowercase each word.\n",
    "  <li> remove the non-letter characters from each word (you can use the function $remove\\_non\\_letters$ defined in Exercise 1).\n",
    "  <li> remove empty words.\n",
    "  <li> remove the stopwords.\n",
    "  <li> remove duplicate words.\n",
    "</ol>\n",
    "</font>\n",
    "</p>\n",
    "<hr style=\"border:solid 2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "P4QgoDEAJu6y",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data/bbc/entertainment/003.txt\n",
      "data/bbc/entertainment/002.txt\n",
      "data/bbc/entertainment/005.txt\n",
      "data/bbc/sport/004.txt\n",
      "data/bbc/politics/001.txt\n",
      "data/bbc/tech/004.txt\n",
      "data/bbc/tech/006.txt']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "INPUT:\n",
    "        files: RDD, each element is (f, content), where f is the name of a file in the collection and content is \n",
    "                its content.\n",
    "        stopwords: a Python list containing the stopwords.\n",
    "\n",
    "OUTPUT:y\n",
    "\n",
    "        a RDD, each element is (w, L), where w is a word and L is the list of the names of the files containing\n",
    "        w (without repetition).\n",
    "\n",
    "'''\n",
    "\n",
    "def inverted_index(files, stopwords):\n",
    "    '''############## WRITE YOUR CODE HERE ##############'''\n",
    "\n",
    "    #Split the content of each file into its constituent words and lowercase each word.\n",
    "    line_split = files.map(lambda x: (x[0],x[1].lower().split()))      \n",
    "    #remove the non-letter characters from each word (you can use the function  ð‘Ÿð‘’ð‘šð‘œð‘£ð‘’_ð‘›ð‘œð‘›_ð‘™ð‘’ð‘¡ð‘¡ð‘’ð‘Ÿð‘   defined in Exercise 1).\n",
    "    no_non_letters = line_split.map(lambda x: (x[0],[remove_non_letters(i) for i in x[1]])) \n",
    "    #remove empty words\n",
    "    no_empty = no_non_letters.map(lambda x: (x[0],[i for i in x[1] if i != \"\"])) \n",
    "    #remove the stopwords\n",
    "    no_stop_words = no_empty.map(lambda x: (x[0],[i for i in x[1] if i not in stopwords])) \n",
    "    #remove duplicate words.\n",
    "    duplicates_removed = no_stop_words.map(lambda x: (x[0],list(set(x[1])))) \n",
    "    #produce the inverted index dictionary\n",
    "    output = duplicates_removed.flatMap(lambda x: [(i,x[0]) for i in x[1]]).reduceByKey(lambda x,y:x+y)\n",
    "    return output\n",
    "\n",
    "\n",
    "    '''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "'''\n",
    "INPUT:\n",
    "        iindex: RDD containing the inverted index, as returned by the function inverted_index.\n",
    "        word: a word.\n",
    "\n",
    "OUTPUT:\n",
    "        prints the list of the files contain the given word.\n",
    "'''\n",
    "        \n",
    "def lookup(iindex, word):\n",
    "    ld = iindex.sortByKey().lookup(word)\n",
    "    if len(ld) > 0:\n",
    "        for i in str(ld).split(\"file\"):\n",
    "            print(\"\".join(re.findall(r\"assignment_2/(.*)\",i)))\n",
    "    else:\n",
    "        print(\"No documents contain the word '\",word,\"'\")\n",
    "\n",
    "####################   GOOD TO KNOW  ####################\n",
    "# The Spark function wholeTextFiles loads into a RDD the content of the text files contained\n",
    "# in the given directory.\n",
    "# Each item of the RDD is a pair (f, content), where f is the name of a file and content is the content\n",
    "# of the file.\n",
    "#######################################################\n",
    "file_collection = sc.wholeTextFiles(\"./data/bbc/*\")        \n",
    "iindex = inverted_index(file_collection, stopwords)\n",
    "lookup(iindex, \"family\")\n",
    "\n",
    "################# EXPECTED OUTPUT #################\n",
    "#\n",
    "# data/bbc/entertainment/002.txt\n",
    "# data/bbc/entertainment/003.txt\n",
    "# data/bbc/entertainment/005.txt\n",
    "# data/bbc/politics/001.txt\n",
    "# data/bbc/sport/004.txt\n",
    "# data/bbc/tech/004.txt\n",
    "# data/bbc/tech/006.txt\n",
    "#\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "bjX3OKP-Ju6z"
   },
   "source": [
    "<hr style=\"border:solid 2px;\">\n",
    "\n",
    "##  Exercise 2\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Given the BBC collection, we want to calculate the **co-occurrence matrix** $M$, such that $M[w_1][w_2]$ is the number of documents in which two words $w_1$ and $w_2$ appear in the same document (it does not matter if they are consecutive or not).\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The function $co\\_occurrence\\_matrix()$ has the following input and output:\n",
    "<ul>\n",
    " <li> **Input.** A RDD $files$ and a Python list $stopwords$, as in the previous exercise.\n",
    " <li> **Output.** A RDD, where each element is $((w_1, w_2), occ)$, where $w_1$ and $w_2$ are words and $occ$ is the number of files in which the two words appear together.\n",
    "</ul>\n",
    "As in the case of the function $inverted\\_index()$, words must be lowercases, non-letter characters, empty words and stopwords should be removed.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Write the code of the function $co\\_occurrence\\_matrix()$. You can draw inspiration from the MapReduce algorithms that we discussed in class. Also, you can use the already implemented function $create\\_pairs()$ to generate all the possible pairs from a list of words. The function assumes that the words in the input list are sorted lexicographically.**\n",
    "<br>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\"border:solid 2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "FkIIh02tJu6z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('also', 'said'), 24),\n",
       " (('new', 'said'), 19),\n",
       " (('said', 'world'), 18),\n",
       " (('said', 'year'), 17),\n",
       " (('also', 'world'), 16),\n",
       " (('last', 'said'), 15),\n",
       " (('one', 'said'), 15),\n",
       " (('said', 'set'), 15),\n",
       " (('said', 'years'), 14),\n",
       " (('said', 'time'), 14)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "INPUT:\n",
    "        words: Python list containing words. IMPORTANT: the function assumes that the \n",
    "        list is sorted in lexicographic order.\n",
    "OUTPUT:\n",
    "        Python list containing all possible pairs from the given list.\n",
    "'''\n",
    "def create_pairs(words):\n",
    "    n = len(words)\n",
    "    output = []\n",
    "    for i in range(0, n):\n",
    "        for j in range(i+1, n):\n",
    "            if ord(words[i][0]) < ord(words[j][0]):\n",
    "                output.append((words[i], words[j]))\n",
    "            else:\n",
    "                output.append((words[j], words[i]))\n",
    "    return output\n",
    "\n",
    "'''\n",
    "INPUT:\n",
    "        files: RDD, each item is (f, content), where f is the name of a file and line is the content of the file.\n",
    "        stopwords: A RDD, each item is ((w1, w2), occ), where w1 and w2 are words and occ is the number of\n",
    "                    files in which w1 and w2 appear together.\n",
    "'''\n",
    "def co_occurrence_matrix(files, stopwords):\n",
    "    '''############## WRITE YOUR CODE HERE ##############'''\n",
    "    \n",
    "    cleaned = files.map(lambda x: (x[0],x[1].lower().split(\" \")))\\\n",
    "                .map(lambda x: (x[0],[remove_non_letters(i) for i in x[1]]))\\\n",
    "                .map(lambda x: (x[0],[i for i in x[1] if i != \"\"]))\\\n",
    "                .map(lambda x: (x[0],[i for i in x[1] if i not in stopwords]))\\\n",
    "                .map(lambda x: (x[0],list(set(x[1]))))\n",
    "    output = cleaned.map(lambda x: (create_pairs(x[1])))\\\n",
    "                    .flatMap(lambda x: x)\\\n",
    "                    .map(lambda x: (x,1))\\\n",
    "                    .reduceByKey(lambda x,y:x+y)\n",
    "    return output\n",
    "\n",
    "    '''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "\n",
    "file_collection = sc.wholeTextFiles(\"./data/bbc/*\")\n",
    "output = co_occurrence_matrix(file_collection, stopwords)    \n",
    "output.takeOrdered(10, key = lambda x: -x[1])\n",
    "\n",
    "################# EXAMPLE OF FORMAT FOR THE EXPECTED OUTPUT #################\n",
    "################# THIS IS NOT THE SOLUTION #################\n",
    "#\n",
    "#[(('a', 'b'), 3),\n",
    "# (('c', 'f'), 12),\n",
    "# ... ]\n",
    "#\n",
    "###################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "6IOoMHo1Ju61"
   },
   "source": [
    "<hr style=\" border:solid 2px;\">\n",
    "\n",
    "##  Exercise 3 - OPTIONAL - enjoy with what you just wrote\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We want to code a function $term\\_freq$ that computes the frequency of each word in a \n",
    "text document. \n",
    "More precisely, given a document $d$ and a word $w$ in that document, we want to \n",
    "compute its frequency $tf(w, d)$, as follows:\n",
    "    \n",
    "<p>    \n",
    "$$ tf(w, d) = \\frac{f_{w, d}}{\\sum\\limits_{w^\\prime \\in d} f_{w^\\prime, d}}$$\n",
    "</p>\n",
    "\n",
    "where $f_{w, d}$ is the number of occurrences of word $w$ in $d$.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<font size=\"3\">\n",
    "The function $term\\_freq$ has the following input and output:\n",
    "<ul>\n",
    "<li> **Input.** A RDD $words$, where each element is a word in a text document $d$ (pre-processing already done).\n",
    "<li> **Output.** A RDD, where each element is a key-value pair $(w, tf(w, d))$.\n",
    "</ul>\n",
    "</font>\n",
    "</p>\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Write the code of the function $term\\_freq$. You can take advantage of the \n",
    "    function $word\\_count$.**\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:solid 2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "euZPF61WJu61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('labour', 0.004),\n",
       " ('plans', 0.02),\n",
       " ('maternity', 0.037),\n",
       " ('pay', 0.045),\n",
       " ('rise', 0.008)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def term_freq(words):\n",
    "    '''############## WRITE YOUR CODE HERE ##############'''\n",
    "    \n",
    "    cleaned = text.map(lambda x: x.lower().split(\" \"))\\\n",
    "              .flatMap(lambda x: [remove_non_letters(i) for i in x])\\\n",
    "              .filter(lambda x: len(x) > 0)\\\n",
    "              .filter(lambda x: x not in stopwords)\n",
    "    \n",
    "    n = cleaned.count()\n",
    "    \n",
    "    raw_count = cleaned.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
    "    \n",
    "    precentages = raw_count.map(lambda x: (x[0],round(x[1]/n,3)))\n",
    "\n",
    "    return precentages\n",
    "\n",
    "    '''############## END OF THE EXERCISE ##############'''\n",
    " \n",
    "text = sc.textFile('./data/bbc/politics/001.txt')\n",
    "words = preprocess(text, stopwords)\n",
    "tf = term_freq(words)\n",
    "tf.take(5)\n",
    "\n",
    "################# EXPECTED OUTPUT #################\n",
    "################# THIS IS NOT THE SOLUTION #################\n",
    "#\n",
    "#[('a', 0,333),\n",
    "# ('b', 0.032),\n",
    "# ... ]\n",
    "#\n",
    "###################################################"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "ass2_spark.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "name": "BE4-Spark.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
